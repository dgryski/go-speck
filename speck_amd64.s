// Code generated by command: go run asm.go -out speck_amd64.s. DO NOT EDIT.

#include "textflag.h"

// func EncryptASM(pt []uint64, ct []uint64, k []uint64)
TEXT ·EncryptASM(SB), NOSPLIT, $0-72
	MOVQ pt_base+0(FP), AX
	MOVQ (AX), CX
	MOVQ 8(AX), AX
	MOVQ k_base+48(FP), DX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ (DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 8(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 16(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 24(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 32(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 40(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 48(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 56(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 64(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 72(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 80(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 88(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 96(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 104(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 112(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 120(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 128(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 136(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 144(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 152(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 160(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 168(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 176(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 184(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 192(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 200(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 208(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 216(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 224(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 232(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 240(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ 248(DX), AX
	ROLQ $0x03, CX
	XORQ AX, CX
	MOVQ ct_base+24(FP), DX
	MOVQ CX, (DX)
	MOVQ AX, 8(DX)
	RET

// func ExpandEncryptASM(pt []uint64, ct []uint64, k []uint64)
TEXT ·ExpandEncryptASM(SB), NOSPLIT, $0-72
	MOVQ pt_base+0(FP), AX
	MOVQ (AX), CX
	MOVQ 8(AX), AX
	MOVQ k_base+48(FP), DX
	MOVQ (DX), BX
	MOVQ 8(DX), DX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x00, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x01, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x02, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x03, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x04, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x05, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x06, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x07, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x08, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x09, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x0a, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x0b, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x0c, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x0d, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x0e, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x0f, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x10, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x11, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x12, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x13, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x14, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x15, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x16, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x17, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x18, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x19, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x1a, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x1b, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x1c, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x1d, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	RORQ $0x08, DX
	ADDQ BX, DX
	XORQ $0x1e, DX
	ROLQ $0x03, BX
	XORQ DX, BX
	RORQ $0x08, AX
	ADDQ CX, AX
	XORQ BX, AX
	ROLQ $0x03, CX
	XORQ AX, CX
	MOVQ ct_base+24(FP), DX
	MOVQ CX, (DX)
	MOVQ AX, 8(DX)
	RET
